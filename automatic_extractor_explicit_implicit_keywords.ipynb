{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Extraction of Explicit and Implicit Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) Extract Relevant Expressions (REs) from a set of several documents, by using LocalMaxs extractor you have implemented. Create adequate criteria to select the most informative REs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 20 documents and create corpus out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from local_max_all_metrics import tokenize, local_max, calculate_n_gram_frequencies, minimum_frequency_filter, special_characters_filter, SPECIAL_CHARACTERS, mi_f\n",
    "import os\n",
    "import math\n",
    "from copy import copy\n",
    "\n",
    "max_re_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fil_2269', 'fil_191', 'fil_131', 'fil_3051', 'fil_688', 'fil_775', 'fil_2439', 'fil_1238', 'fil_919', 'fil_4561', 'fil_2036', 'fil_650', 'fil_3278', 'fil_569', 'fil_4568', 'fil_3289', 'fil_36', 'fil_4007', 'fil_1977'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS_PATH = \"./corpus2mw\"\n",
    "CORPUS_NAME = 'corpus2mw'\n",
    "\n",
    "corpus_total = defaultdict(tuple)\n",
    "n_gram_freq_dict_per_doc = defaultdict(tuple)\n",
    "doc_names = []\n",
    "# open all files starting with \"fil\" in corpus path\n",
    "doc_count = 1\n",
    "for doc_path in Path(CORPUS_PATH).glob('fil_*'):\n",
    "    corpus_per_doc = defaultdict(tuple)\n",
    "    # read whole document and strip new lines\n",
    "    doc = Path(doc_path).read_text().replace('\\n', '')\n",
    "    # standardize space characters\n",
    "    doc = doc.replace(u'\\xa0', u' ').replace(u'\\u3000', u' ').replace(u'\\u2009', u' ')\n",
    "    \n",
    "    doc_name = os.path.basename(doc_path)\n",
    "    doc_names.append(doc_name)\n",
    "    corpus_total[doc_name] = tokenize(doc)\n",
    "    corpus_per_doc[doc_name] = tokenize(doc)\n",
    "    n_gram_freq_dict_doc = calculate_n_gram_frequencies(corpus_per_doc, max_re_size + 1, len(corpus_total[doc_name]))\n",
    "    filtered_n_gram_freq_dict_doc = minimum_frequency_filter(n_gram_freq_dict_doc, 2)\n",
    "    filtered_n_gram_freq_dict_doc = special_characters_filter(filtered_n_gram_freq_dict_doc, SPECIAL_CHARACTERS)\n",
    "    n_gram_freq_dict_per_doc[doc_name] = filtered_n_gram_freq_dict_doc\n",
    "\n",
    "    doc_count = doc_count + 1\n",
    "    if doc_count == 20: break\n",
    "\n",
    "corpus_words = [word for doc_list in corpus_total.values() for word in doc_list]\n",
    "corpus_total_size = len(corpus_words)\n",
    "n_gram_freq_dict_per_doc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_total['fil_2269'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter-Implementation to select most informative REs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return only res which appear at least min_freq times in each document\n",
    "def min_freq(re_per_doc, min_freq):\n",
    "    best_re_per_doc = defaultdict(tuple)\n",
    "    for doc_name in re_per_doc.keys():\n",
    "        frequent_re = list(filter(lambda x:x['abs_freq'] >= min_freq, re_per_doc[doc_name]))[:10]\n",
    "        best_re_per_doc[doc_name] = frequent_re\n",
    "    return best_re_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the tf_idf for each res inside of each document and return the res sorted from highest to lowest\n",
    "\n",
    "def tf_idf_val(re_abs_freq, document_size, amount_of_docs, amount_of_docs_with_re):\n",
    "    return (re_abs_freq / document_size) * math.log(amount_of_docs / amount_of_docs_with_re)\n",
    "\n",
    "def get_amount_of_docs_with_re(re, re_per_doc):\n",
    "    re_tup = re['re']\n",
    "    count = 0\n",
    "    for doc_name in re_per_doc.keys():\n",
    "        re_tups = list(map(lambda x:x['re'], re_per_doc[doc_name]))\n",
    "        if re_tup in re_tups: count += 1\n",
    "    return count\n",
    "    \n",
    "\n",
    "def tf_idf(re_per_doc, doc_amount):\n",
    "    re_per_doc_with_tfidf_val = copy(re_per_doc)\n",
    "    best_re_per_doc = defaultdict(tuple)\n",
    "    for doc_name in re_per_doc.keys():\n",
    "        re_obs_with_tfidf_val = []\n",
    "        for re_ob in re_per_doc[doc_name]:\n",
    "            #calculate tf_idf value\n",
    "            re_abs_freq = re_ob['abs_freq']\n",
    "            document_size = len(corpus_total[doc_name])\n",
    "            amount_of_docs_with_re = get_amount_of_docs_with_re(re_ob, re_per_doc)\n",
    "            tf_idf_of_re = tf_idf_val(re_abs_freq, document_size, doc_amount, amount_of_docs_with_re)\n",
    "            # update object(s)\n",
    "            re_ob_with_tfidf_val = copy(re_ob)\n",
    "            re_ob_with_tfidf_val['tf_idf'] = tf_idf_of_re\n",
    "            re_obs_with_tfidf_val.append(re_ob_with_tfidf_val)\n",
    "        re_per_doc_with_tfidf_val[doc_name] = re_obs_with_tfidf_val\n",
    "    \n",
    "    # sorting\n",
    "    for doc_name in re_per_doc_with_tfidf_val.keys():\n",
    "        re_per_doc_with_tfidf_val[doc_name] = sorted(re_per_doc_with_tfidf_val[doc_name], key=lambda x: x['tf_idf'], reverse=True)[:10]\n",
    "    return re_per_doc_with_tfidf_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute relevant expressions for documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_freq_dict = calculate_n_gram_frequencies(corpus_total, max_re_size + 1, corpus_total_size)\n",
    "\n",
    "filtered_n_gram_freq_dict = minimum_frequency_filter(n_gram_freq_dict, 2)\n",
    "filtered_n_gram_freq_dict = special_characters_filter(filtered_n_gram_freq_dict, SPECIAL_CHARACTERS)\n",
    "relevant_expressions = local_max(corpus_total, max_re_size, filtered_n_gram_freq_dict, mi_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# calculate n-gram frequencies for every document\n",
    "re_per_doc = defaultdict(tuple)\n",
    "for doc_name in doc_names:\n",
    "    re_per_doc[doc_name] = []\n",
    "    for re_len in range(0, max_re_size):\n",
    "        n_grams_with_specific_size_of_doc = n_gram_freq_dict_per_doc[doc_name][re_len+1].keys()\n",
    "        # which relevant expressions extracted over the corpus are in our current document?\n",
    "        re_with_specific_size_of_doc = [re for re in relevant_expressions if re in n_grams_with_specific_size_of_doc]\n",
    "        for re in re_with_specific_size_of_doc:\n",
    "            re_per_doc[doc_name].append({'re': re, 'abs_freq': n_gram_freq_dict_per_doc[doc_name][re_len+1][re]['abs_freq']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-79f956e8693d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate most relevant relevant expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_re_per_doc_minfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_per_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_re_per_doc_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_per_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_re_per_doc_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_re_per_doc_minfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1e2942805e88>\u001b[0m in \u001b[0;36mtf_idf\u001b[0;34m(re_per_doc, doc_amount)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#calculate tf_idf value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mre_abs_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_ob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abs_freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mdocument_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mamount_of_docs_with_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_amount_of_docs_with_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_ob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre_per_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtf_idf_of_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_abs_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_amount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount_of_docs_with_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate most relevant relevant expressions\n",
    "best_re_per_doc_minfreq = min_freq(re_per_doc,3)\n",
    "best_re_per_doc_tfidf = tf_idf(re_per_doc, 20)\n",
    "print(best_re_per_doc_tfidf)\n",
    "print(best_re_per_doc_minfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"John F. Kennedy said in the United States of America that the United States of America is a fun park\"\n",
    "\n",
    "def get_highest_word_similarity(words):\n",
    "    #create a loop comparing each entry in x with every other entry in x\n",
    "    similarities = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokens = nlp(words)\n",
    "    for token1 in range(len(tokens)):\n",
    "        for token2 in range(token1+1,len(tokens)):\n",
    "            similarities.append([tokens[token1].similarity(tokens[token2]),tokens[token1],tokens[token2]])\n",
    "    #return the similarities sorted by highest to lowest\n",
    "    return sorted(similarities, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_highest_word_similarity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
